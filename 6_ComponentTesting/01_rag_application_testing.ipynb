{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21320bc",
   "metadata": {},
   "source": [
    "### Component Of RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eec299",
   "metadata": {},
   "source": [
    "#### Create a simple Rag Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcacaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-chroma\n",
    "!pip install -U DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f39d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from deepeval.tracing import observe, update_current_span\n",
    "import os\n",
    "os.environ[\"CONFIDENT_TRACE_FLUSH\"] = \"YES\"\n",
    "\n",
    "@observe(type=\"llm\", model=\"llama3.2:latest\")\n",
    "def get_local_llm(model_name):\n",
    "    return ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=model_name,\n",
    "        temperature=0.5,\n",
    "        max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b24ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Web\n",
    "from langchain_community.document_loaders import SeleniumURLLoader\n",
    "loader = SeleniumURLLoader(urls=[\"https://www.descope.com/learn/post/mcp\"])\n",
    "data = loader.load()\n",
    "# Convert Data Into Chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                               chunk_overlap=150,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"])\n",
    "docs = text_splitter.split_documents(data)\n",
    "# Create Embedding and Vector Store\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "# Note: Ensure Ollama is running and has the model available\n",
    "ollama_embedding = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "vector_store = Chroma.from_documents(documents=docs, \n",
    "                                     embedding= ollama_embedding)\n",
    "## Setting up retrieval augmented generation (RAG)\n",
    "retriever = vector_store.as_retriever()\n",
    "## Initialize LLM For Generation\n",
    "llm = get_local_llm(\"llama3.2:latest\")\n",
    "# Define Prompt Template \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = \"\"\"\n",
    "Your are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know, just say you don't know.\n",
    "keep the answer concise.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "# Construct Rag Chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser, SimpleJsonOutputParser\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c30861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric, ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "@observe(type=\"retriever\", \n",
    "         name=\"Retriever\",\n",
    "         metrics=[ContextualRelevancyMetric()], \n",
    "         embedder=\"llama3.2:latest\")\n",
    "def retrieve_context(question):\n",
    "    docs = retriever.invoke(question)\n",
    "    print(f\"Retrieved {len(docs)} documents.\")\n",
    "    retrieved_contexts = [doc.page_content for doc in docs if doc.page_content is not None]\n",
    "    update_current_span(\n",
    "        test_case=LLMTestCase(input=question, retrieval_context=retrieved_contexts)\n",
    "    )\n",
    "    return retrieved_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ccb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(type=\"rag_application\", \n",
    "         name=\"RAG Application\", \n",
    "         metrics=[ContextualRelevancyMetric(), AnswerRelevancyMetric()])\n",
    "def rag_application(question):\n",
    "    actual_response = rag_chain.invoke(question)\n",
    "    print(\"RAG Response:\", actual_response)\n",
    "    context_list = retrieve_context(question)\n",
    "    print(\"Retrieved Context:\", context_list)\n",
    "    update_current_span(\n",
    "        test_case=LLMTestCase(\n",
    "            input=question,\n",
    "            actual_output=actual_response,\n",
    "            retrieval_context=context_list\n",
    "        )\n",
    "    )\n",
    "    return actual_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed277a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_application(\"What is MCP?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed04a61",
   "metadata": {},
   "source": [
    "#### Evaluation of RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31817e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import Golden\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "goldens = [Golden(input=\"What is MCP\")]\n",
    "dataset = EvaluationDataset(goldens=goldens)\n",
    "for golden in dataset.evals_iterator():\n",
    "     rag_application(golden.input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
