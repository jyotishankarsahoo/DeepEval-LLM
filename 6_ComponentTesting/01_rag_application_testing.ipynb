{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21320bc",
   "metadata": {},
   "source": [
    "### Component Of RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eec299",
   "metadata": {},
   "source": [
    "#### Create a simple Rag Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcacaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-chroma\n",
    "!pip install -U DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "137a4e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f39d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import deepeval\n",
    "from deepeval.tracing import observe, update_current_span\n",
    "import os\n",
    "os.environ[\"CONFIDENT_TRACE_FLUSH\"] = \"YES\"\n",
    "\n",
    "@observe(type=\"llm\", model=\"llama3.2:latest\")\n",
    "def get_local_llm(model_name):\n",
    "    return ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=model_name,\n",
    "        temperature=0.5,\n",
    "        max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c2b24ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Web\n",
    "from langchain_community.document_loaders import SeleniumURLLoader\n",
    "loader = SeleniumURLLoader(urls=[\"https://www.descope.com/learn/post/mcp\"])\n",
    "data = loader.load()\n",
    "# Convert Data Into Chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                               chunk_overlap=150,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"])\n",
    "docs = text_splitter.split_documents(data)\n",
    "# Create Embedding and Vector Store\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "# Note: Ensure Ollama is running and has the model available\n",
    "ollama_embedding = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "vector_store = Chroma.from_documents(documents=docs, \n",
    "                                     embedding= ollama_embedding)\n",
    "## Setting up retrieval augmented generation (RAG)\n",
    "retriever = vector_store.as_retriever()\n",
    "## Initialize LLM For Generation\n",
    "llm = get_local_llm(\"llama3.2:latest\")\n",
    "# Define Prompt Template \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = \"\"\"\n",
    "Your are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know, just say you don't know.\n",
    "keep the answer concise.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "# Construct Rag Chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser, SimpleJsonOutputParser\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c399337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Model Context Protocol (MCP) is a standardized way for LLMs and AI agents to connect with external data sources and tools. It allows the server to return requested information to the client in a standardized format, which is then integrated into the agent\\'s understanding of the conversation. This process enables the agent to generate responses based on current data, creating an unobtrusive experience where it appears to \"know\" information it couldn\\'t possibly have from its training data alone.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"How MCP works?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ccb0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
