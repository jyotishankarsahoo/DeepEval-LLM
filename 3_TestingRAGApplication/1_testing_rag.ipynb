{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78cf0446",
   "metadata": {},
   "source": [
    "#### Install langchain chroma for storage vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83526c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01aa1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662c898",
   "metadata": {},
   "source": [
    "#### Create a RAG System To Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import SeleniumURLLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e2505f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up the Ollama LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:8b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.5,\n",
    "    num_predict=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28364e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading data from web page\n",
    "loader = SeleniumURLLoader(urls=[\"https://www.descope.com/learn/post/mcp\"])\n",
    "data = loader.load()\n",
    "## Splitting the data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \n",
    "                                               chunk_overlap=100, \n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"])\n",
    "splits = text_splitter.split_documents(data)\n",
    "## Creating embeddings and vector store\n",
    "embedding = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "vector_db = Chroma.from_documents(splits, embedding)\n",
    "## Setting up retrieval augmented generation\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "## Creating prompt template\n",
    "template = \"\"\"\n",
    "    Answer the question based only on following context:\n",
    "    {context}\n",
    "    Give a summary not the full details\n",
    "    Question: {question}\n",
    "    Instructions:\n",
    "        - Answer based ONLY on the information provided in the context above\n",
    "        - If the context doesn't contain enough information to answer the question, say so\n",
    "        - Be accurate and precise in your response\n",
    "        - Give a summary, not full details\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "## Function to format documents\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "## Function to retrieve and format the prompt\n",
    "def retrieve_and_format(question: str) -> List[Document]:\n",
    "    docs = retriever.invoke(question)\n",
    "    # print(f\"Number of documents retrieved: {len(docs)}\")\n",
    "    # print(\"--- Retrieved Document Content ---\")\n",
    "    # for doc in docs:\n",
    "    # # Print the content and metadata source\n",
    "    #     print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    #     print(doc.page_content[:300] + \"...\\n\")\n",
    "    return format_docs(docs)\n",
    "## Final runnable chain\n",
    "rag_chain = {\"context\": retrieve_and_format, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c37c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, MCP (Model Context Protocol) is a protocol designed to connect AI applications like Claude Desktop to external tools and resources.\n",
      "\n",
      "Key points from the summary:\n",
      "\n",
      "1.  **Purpose:** To allow AI apps to utilize capabilities (tools, resources, prompts) offered by external servers.\n",
      "2.  **Mechanism:** It involves a handshake/connection, capability discovery (asking the server what it offers), and registration of those capabilities.\n",
      "3.  **Benefit:** It builds upon function calling (the standard way LLMs call\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is MCP\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72ce4e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context:\n",
      "\n",
      "Function calling is the primary method used to call APIs from LLMs. MCP builds upon this concept by standardizing the way tools (functions) are specified, discovered, and executed across different AI systems and models. Essentially, MCP provides a universal protocol for implementing function-like capabilities in AI apps, making it compatible with various model vendors unlike some original function calling implementations which might have been more limited in scope.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is relationship between function calling and Model context protocol\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1e4e0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here is a summary of the MCP architecture:\n",
      "\n",
      "The MCP architecture involves an **AI application** (like Claude Desktop) acting as the **Client**, and the **external tools/resources/APIs** acting as the **Servers**.\n",
      "\n",
      "The core process is a **Protocol Handshake**:\n",
      "\n",
      "1.  **Initial Connection:** The client connects to the configured MCP servers.\n",
      "2.  **Capability Discovery:** The client asks the server what capabilities/tools/resources it offers.\n",
      "3.  **Registration:** The client registers the discovered capabilities, making them available for the AI to use.\n",
      "\n",
      "This architecture connects AI applications to external context and builds upon standard function calling methods to simplify and standardize development, aiming to solve issues like excessive maintenance and fragmented implementation.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is the Architecture of MCP\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71ee79",
   "metadata": {},
   "source": [
    "#### Testing RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2994f0c3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
