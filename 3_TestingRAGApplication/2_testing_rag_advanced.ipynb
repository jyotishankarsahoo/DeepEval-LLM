{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78cf0446",
   "metadata": {},
   "source": [
    "#### Install langchain chroma for storage vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83526c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01aa1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662c898",
   "metadata": {},
   "source": [
    "#### Create a RAG System To Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import SeleniumURLLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2505f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up the Ollama LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:8b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.5,\n",
    "    num_predict=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28364e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading data from web page\n",
    "loader = SeleniumURLLoader(urls=[\"https://www.descope.com/learn/post/mcp\"])\n",
    "data = loader.load()\n",
    "## Splitting the data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \n",
    "                                               chunk_overlap=100, \n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"])\n",
    "splits = text_splitter.split_documents(data)\n",
    "## Creating embeddings and vector store\n",
    "embedding = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "vector_db = Chroma.from_documents(splits, embedding)\n",
    "## Setting up retrieval augmented generation\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "## Creating prompt template\n",
    "template = \"\"\"\n",
    "    Answer the question based only on following context:\n",
    "    {context}\n",
    "    Give a summary not the full details\n",
    "    Question: {question}\n",
    "    Instructions:\n",
    "        - Answer based ONLY on the information provided in the context above\n",
    "        - If the context doesn't contain enough information to answer the question, say so\n",
    "        - Be accurate and precise in your response\n",
    "        - Give a summary, not full details\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "## Function to format documents\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "## Function to retrieve and format documents based on question\n",
    "def retrieve_and_format(question: str) -> List[Document]:\n",
    "    docs = retriever.invoke(question)\n",
    "    return format_docs(docs)\n",
    "## Function to retrieve documents based on question\n",
    "def retrieve_documents(question: str) -> List[str]:\n",
    "    docs = retriever.invoke(question)\n",
    "    return [doc.page_content for doc in docs]\n",
    "## Final runnable chain\n",
    "rag_chain = {\"context\": retrieve_and_format, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cac68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models import OllamaModel\n",
    "local_llm = OllamaModel(\n",
    "    model=\"deepseek-r1:8b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.5,\n",
    "    generation_kwargs={\"num_predict\": 500}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09097412",
   "metadata": {},
   "source": [
    "#### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd003233",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"input\": \"What is MCP\",\n",
    "        \"expected_output\": \n",
    "            \"\"\"\n",
    "            MCP, or Model Context Protocol, is a protocol used by an MCP client like Claude Desktop.\n",
    "            It connects AI applications (like Claude Desktop) to external tools and resources.\n",
    "            It builds upon the standard function calling method used by large language models (LLMs) to call APIs, aiming to simplify development and provide a more consistent way for AI apps to access external context.\n",
    "            The process involves a handshake, capability discovery, and registration.\n",
    "            \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is relationship between function calling and Model context protocol\",\n",
    "        \"expected_output\": \n",
    "            \"\"\"\n",
    "            The Model Context Protocol (MCP) builds upon function calling, which is the standard method for LLMs to call APIs.\n",
    "            However, MCP enhances this by standardizing how tools (functions) are specified, discovered, and executed across different AI systems.\n",
    "            This standardization allows for plug-and-play integration without custom code, addressing limitations like excessive maintenance and fragmentation found with traditional function calling approaches tied to specific vendors or ecosystems.\n",
    "            \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the Architecture of MCP\",\n",
    "        \"expected_output\": \n",
    "            \"\"\"\n",
    "            The architecture of MCP involves an **MCP client** (like Claude Desktop) interacting with **MCP servers**. The process includes:\n",
    "                1.  **Protocol Handshake:** The client connects to the configured MCP servers.\n",
    "                2.  **Capability Discovery:** The client queries the server for its available tools, resources, and prompts.\n",
    "                3.  **Registration:** The client registers the discovered capabilities, making them available for the AI to use.\n",
    "            This architecture connects AI applications (clients) to external tools and resources (servers) via a standardized method (the protocol), building upon the concept of function calling to simplify development.\n",
    "            \"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f150fa",
   "metadata": {},
   "source": [
    "#### Creating Golden dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd998cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "\n",
    "goldens = []\n",
    "\n",
    "for data in test_data:\n",
    "    golden = Golden(\n",
    "        input=data[\"input\"],\n",
    "        expected_output=data[\"expected_output\"]\n",
    "    )\n",
    "    goldens.append(golden)\n",
    "\n",
    "data_set = EvaluationDataset(goldens=goldens)\n",
    "data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a779861",
   "metadata": {},
   "source": [
    "#### Pushing Dataset to Confident AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d43357",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.push(alias=\"test_qa_dataset\", finalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc93690",
   "metadata": {},
   "source": [
    "#### Pulling from Confident AI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_dataset = EvaluationDataset()\n",
    "cloud_dataset.pull(alias=\"test_qa_dataset\")\n",
    "print(cloud_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac90cc9",
   "metadata": {},
   "source": [
    "#### Prepare `actual_output` and `retrieval_context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea150864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "## Its is going to use the LLM and Vector Database stored information (RAG)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "actual_output = qa_chain.invoke(\"What is MCP\")\n",
    "actual_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_output_with_context(question: str):\n",
    "    actual_output = qa_chain.run(question)\n",
    "    retrieved_document = retrieve_documents(question)\n",
    "    return actual_output, retrieved_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_output, retrieved_context = get_actual_output_with_context(\"What is MCP\")\n",
    "print(f\"###ACTUAL OUTPUT### /n:{actual_output}/n\")\n",
    "print(f\"###RETRIEVAL CONTEXT### /n: {retrieved_context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd49ceae",
   "metadata": {},
   "source": [
    "#### Creating LLM Test Case with Goldens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d47ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import Golden\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from typing import List\n",
    "def convert_golden_to_testCase(goldens: List[Golden]) -> List[LLMTestCase]:\n",
    "    test_cases = []\n",
    "    for golden in goldens:\n",
    "        actual_output, retrieved_context = get_actual_output_with_context(golden.input)\n",
    "        test_case = LLMTestCase(\n",
    "            input = golden.input,\n",
    "            actual_output = actual_output,\n",
    "            expected_output = golden.expected_output,\n",
    "            retrieval_context = retrieved_context\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases\n",
    "data = convert_golden_to_testCase(cloud_dataset.goldens)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88926dc",
   "metadata": {},
   "source": [
    "#### Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7365f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepeval.metrics\n",
    "from deepeval.config import settings\n",
    "settings.DEEPEVAL_PER_ATTEMPT_TIMEOUT_SECONDS_OVERRIDE = \"800\"\n",
    "deepeval.evaluate(\n",
    "    data,\n",
    "    metrics=[\n",
    "        deepeval.metrics.AnswerRelevancyMetric(model=local_llm),\n",
    "        deepeval.metrics.FaithfulnessMetric(model=local_llm),\n",
    "        deepeval.metrics.ContextualPrecisionMetric(model=local_llm),\n",
    "        deepeval.metrics.ContextualRelevancyMetric(model=local_llm)\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
