{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b47cdd",
   "metadata": {},
   "source": [
    "### Testing LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef405677",
   "metadata": {},
   "source": [
    "#### Simple Code using Ollama For Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    prompt = f\"Analyse the sentiment of this text and give me POSITIVE, NEGATIVE and NEUTRAL for this {text}\"\n",
    "    response = ollama.chat(model=\"qwen2.5:latest\", \n",
    "                           messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "tests = [\n",
    "    \"I love this product\", \n",
    "    \"I hate the food in this restaurant\", \n",
    "    \"the movie is just ok\"\n",
    "]\n",
    "\n",
    "for text in tests:\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    print(f\"Text: {text}\\nSentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a138aba",
   "metadata": {},
   "source": [
    "#### Text Summarization (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "    Function calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes referred to as “tool use,” function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works, adding context for the LLM. This is achieved by streaming tool definitions (including their capabilities, data stores, and possible prompts) to LLMs from the MCP server. \n",
    "\n",
    "    Without MCP, when you use a function call directly with an LLM API, you need to:\n",
    "\n",
    "    Define model-specific function schemas, which are JSON descriptions of the function, acceptable parameters, and what it returns.\n",
    "    Implement handlers (the actual code that executes when a function is called) for those functions.\n",
    "    Create different implementations for each model you support.\n",
    "    MCP standardizes this process by:\n",
    "\n",
    "    Defining a consistent way to specify tools (functions) across any AI system.\n",
    "    Providing a protocol for discovering available tools and executing them.\n",
    "    Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.\n",
    "    You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API call with it. While this allows some purpose-built tooling, it’s bound to OpenAI’s ecosystem. MCP brings similar capabilities to any AI application that implements the protocol, regardless of the underlying model vendor.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1860b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def summarize_text(text):\n",
    "    prompt = f\"Please provide a concise summary of the given {text} and keep it as simple as possible\"\n",
    "    response = ollama.chat(model=\"llama3.2:latest\", \n",
    "                           messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "summary = summarize_text(text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fca2c5",
   "metadata": {},
   "source": [
    "#### NLP Library (Transformer) From Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748aaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "# !pip install evaluate\n",
    "# !pip install tensorflow\n",
    "# !pip install tf-keras\n",
    "# !pip install scikit-learn\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I hate this food even though many people says its terrible and bad and worst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3925121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summary = summarizer(text)\n",
    "summary[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a633a",
   "metadata": {},
   "source": [
    "#### Custom LLM Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720dfb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", \n",
    "                      model=\"ExecuteAutomation/bert-base-text-classification-model\")\n",
    "classifier([\"I am struggling with life\", \n",
    "            \"I love testing AI\", \n",
    "            \"I started annoyed with my laptop which has no GPU\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571b6486",
   "metadata": {},
   "source": [
    "#### Zero Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebea1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\"I love learning machine learning from Udemy\", \n",
    "           candidate_labels=[\"education\", \n",
    "                             \"marketing\", \n",
    "                             \"motivation\", \n",
    "                             \"Business\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a9069",
   "metadata": {},
   "source": [
    "#### NER (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6cac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"ner\", grouped_entities=True)\n",
    "classifier(\"Jyoti is working in Apple and living in Sunnyvale, california, USA and my email is jss@apple.com\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
